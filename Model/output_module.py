# -*- coding: utf-8 -*-
"""output_module.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BWTg9NU7M5a5RXX60n4Bb43jX-eSA5Tt
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Callable, Dict, Optional, Sequence, Union
import schnetpack as spk
from schnetpack import properties

class OutFock(nn.Module):
    def __init__(
        self,
        n_in: int,
        n_out: int = 1024,
        n_hidden: Optional[Union[int, Sequence[int]]] = None,
        n_layers: int = 2,
        activation: Callable = F.silu,
        aggregation_mode: str = "sum",
        output_key: str = "fock_matrix",
        per_atom_output_key: Optional[str] = None,
        max_atomic_number: int = 8,
        embedding_size: int = 64,
    ):
        super(OutFock, self).__init__()
        self.output_key = output_key
        self.model_outputs = [output_key]
        self.per_atom_output_key = per_atom_output_key
        if self.per_atom_output_key is not None:
            self.model_outputs.append(self.per_atom_output_key)
        self.n_out = n_out

        self.atomic_number_embedding = nn.Embedding(max_atomic_number + 1, embedding_size)

        self.outnet = spk.nn.build_gated_equivariant_mlp(
            n_in=n_in,
            n_out=n_out,
            n_hidden=n_hidden,
            n_layers=n_layers,
            activation=activation,
            sactivation=activation,
        )
        self.aggregation_mode = aggregation_mode

    def forward(self, inputs):
        atomic_numbers = inputs['scalar_representation']
        positions = inputs['vector_representation']

        a,p = self.outnet((atomic_numbers, positions))

        if self.per_atom_output_key is not None:
            inputs[self.per_atom_output_key] = a

        if self.aggregation_mode is not None:
            idx_m = inputs[properties.idx_m]
            maxm = int(idx_m[-1]) + 1
            a = spk.nn.scatter_add(a, idx_m, dim_size=maxm)
            a = torch.squeeze(a, -1)

            if self.aggregation_mode == "avg":
                a = a / inputs[spk.properties.n_atoms]

        a = a.reshape(-1, 32, 32)
        a = a + torch.transpose(a, -1, -2)
        a = a.reshape(-1)

        inputs[self.output_key] = a
        return inputs